{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62cf89a0",
   "metadata": {},
   "source": [
    "# SPARK SQL PRACTICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8136ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c99bb262",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a1f2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "532aa8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d9e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa16488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 21:08:16,948 WARN util.Utils: Your hostname, shiva-life resolves to a loopback address: 127.0.1.1; using 192.168.1.4 instead (on interface wlo1)\n",
      "2023-05-05 21:08:16,949 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-05-05 21:08:17,868 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSql\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d76e62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    fields = line.split(',')\n",
    "    return Row(ID = int(fields[0]), name = str(fields[1].encode(\"utf-8\")), age = int(fields[2]), numFriends = int(fields[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5086cac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.sparkContext.textFile(\"file:///home/bigdata/workspace/Spark_resources/fakefriends.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3cc7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "people = lines.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "449f4a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 21:08:35,536 ERROR executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigdata/ecosystem/spark/python/pyspark/rdd.py\", line 1563, in takeUpToNumLeft\n",
      "    except StopIteration:\n",
      "  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_4552/4020555056.py\", line -1, in mapper\n",
      "ValueError: invalid literal for int() with base 10: 'ID'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2023-05-05 21:08:35,578 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (192.168.1.4 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/bigdata/ecosystem/spark/python/pyspark/rdd.py\", line 1563, in takeUpToNumLeft\n",
      "    except StopIteration:\n",
      "  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_4552/4020555056.py\", line -1, in mapper\n",
      "ValueError: invalid literal for int() with base 10: 'ID'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2023-05-05 21:08:35,581 ERROR scheduler.TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.1.4 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/bigdata/ecosystem/spark/python/pyspark/rdd.py\", line 1563, in takeUpToNumLeft\n    except StopIteration:\n  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_4552/4020555056.py\", line -1, in mapper\nValueError: invalid literal for int() with base 10: 'ID'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/bigdata/ecosystem/spark/python/pyspark/rdd.py\", line 1563, in takeUpToNumLeft\n    except StopIteration:\n  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_4552/4020555056.py\", line -1, in mapper\nValueError: invalid literal for int() with base 10: 'ID'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4552/3747517468.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Infer the schema, and register the DataFrame as a table.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mschemaPeople\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeople\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mschemaPeople\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"people\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ecosystem/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ecosystem/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ecosystem/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \"\"\"\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ecosystem/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \"\"\"\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[0;32m~/ecosystem/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m         \"\"\"\n\u001b[0;32m-> 1588\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ecosystem/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ecosystem/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ecosystem/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ecosystem/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ecosystem/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.1.4 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/bigdata/ecosystem/spark/python/pyspark/rdd.py\", line 1563, in takeUpToNumLeft\n    except StopIteration:\n  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_4552/4020555056.py\", line -1, in mapper\nValueError: invalid literal for int() with base 10: 'ID'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/bigdata/ecosystem/spark/python/pyspark/rdd.py\", line 1563, in takeUpToNumLeft\n    except StopIteration:\n  File \"/home/bigdata/ecosystem/spark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_4552/4020555056.py\", line -1, in mapper\nValueError: invalid literal for int() with base 10: 'ID'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Infer the schema, and register the DataFrame as a table.\n",
    "\n",
    "schemaPeople = spark.createDataFrame(people).cache()\n",
    "schemaPeople.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6372738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"select ID,name from people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25daf781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(ID=0, name=\"b'Will'\")\n",
      "Row(ID=1, name=\"b'Jean-Luc'\")\n",
      "Row(ID=2, name=\"b'Hugh'\")\n",
      "Row(ID=3, name=\"b'Deanna'\")\n",
      "Row(ID=4, name=\"b'Quark'\")\n",
      "Row(ID=5, name=\"b'Weyoun'\")\n",
      "Row(ID=6, name=\"b'Gowron'\")\n",
      "Row(ID=7, name=\"b'Will'\")\n",
      "Row(ID=8, name=\"b'Jadzia'\")\n",
      "Row(ID=9, name=\"b'Hugh'\")\n",
      "Row(ID=10, name=\"b'Odo'\")\n",
      "Row(ID=11, name=\"b'Ben'\")\n",
      "Row(ID=12, name=\"b'Keiko'\")\n",
      "Row(ID=13, name=\"b'Jean-Luc'\")\n",
      "Row(ID=14, name=\"b'Hugh'\")\n",
      "Row(ID=15, name=\"b'Rom'\")\n",
      "Row(ID=16, name=\"b'Weyoun'\")\n",
      "Row(ID=17, name=\"b'Odo'\")\n",
      "Row(ID=18, name=\"b'Jean-Luc'\")\n",
      "Row(ID=19, name=\"b'Geordi'\")\n",
      "Row(ID=20, name=\"b'Odo'\")\n",
      "Row(ID=21, name=\"b'Miles'\")\n",
      "Row(ID=22, name=\"b'Quark'\")\n",
      "Row(ID=23, name=\"b'Keiko'\")\n",
      "Row(ID=24, name=\"b'Julian'\")\n",
      "Row(ID=25, name=\"b'Ben'\")\n",
      "Row(ID=26, name=\"b'Julian'\")\n",
      "Row(ID=27, name=\"b'Leeta'\")\n",
      "Row(ID=28, name=\"b'Martok'\")\n",
      "Row(ID=29, name=\"b'Nog'\")\n",
      "Row(ID=30, name=\"b'Keiko'\")\n",
      "Row(ID=31, name=\"b'Miles'\")\n",
      "Row(ID=32, name=\"b'Nog'\")\n",
      "Row(ID=33, name=\"b'Dukat'\")\n",
      "Row(ID=34, name=\"b'Jean-Luc'\")\n",
      "Row(ID=35, name=\"b'Beverly'\")\n",
      "Row(ID=36, name=\"b'Kasidy'\")\n",
      "Row(ID=37, name=\"b'Geordi'\")\n",
      "Row(ID=38, name=\"b'Deanna'\")\n",
      "Row(ID=39, name=\"b'Morn'\")\n",
      "Row(ID=40, name=\"b'Odo'\")\n",
      "Row(ID=41, name=\"b'Hugh'\")\n",
      "Row(ID=42, name=\"b'Brunt'\")\n",
      "Row(ID=43, name=\"b'Guinan'\")\n",
      "Row(ID=44, name=\"b'Nerys'\")\n",
      "Row(ID=45, name=\"b'Dukat'\")\n",
      "Row(ID=46, name=\"b'Morn'\")\n",
      "Row(ID=47, name=\"b'Brunt'\")\n",
      "Row(ID=48, name=\"b'Nog'\")\n",
      "Row(ID=49, name=\"b'Ezri'\")\n",
      "Row(ID=50, name=\"b'Quark'\")\n",
      "Row(ID=51, name=\"b'Lwaxana'\")\n",
      "Row(ID=52, name=\"b'Beverly'\")\n",
      "Row(ID=53, name=\"b'Geordi'\")\n",
      "Row(ID=54, name=\"b'Brunt'\")\n",
      "Row(ID=55, name=\"b'Keiko'\")\n",
      "Row(ID=56, name=\"b'Gowron'\")\n",
      "Row(ID=57, name=\"b'Odo'\")\n",
      "Row(ID=58, name=\"b'Hugh'\")\n",
      "Row(ID=59, name=\"b'Morn'\")\n",
      "Row(ID=60, name=\"b'Geordi'\")\n",
      "Row(ID=61, name=\"b'Kasidy'\")\n",
      "Row(ID=62, name=\"b'Keiko'\")\n",
      "Row(ID=63, name=\"b'Jean-Luc'\")\n",
      "Row(ID=64, name=\"b'Elim'\")\n",
      "Row(ID=65, name=\"b'Guinan'\")\n",
      "Row(ID=66, name=\"b'Geordi'\")\n",
      "Row(ID=67, name=\"b'Jadzia'\")\n",
      "Row(ID=68, name=\"b'Guinan'\")\n",
      "Row(ID=69, name=\"b'Jean-Luc'\")\n",
      "Row(ID=70, name=\"b'Brunt'\")\n",
      "Row(ID=71, name=\"b'Geordi'\")\n",
      "Row(ID=72, name=\"b'Kasidy'\")\n",
      "Row(ID=73, name=\"b'Brunt'\")\n",
      "Row(ID=74, name=\"b'Leeta'\")\n",
      "Row(ID=75, name=\"b'Morn'\")\n",
      "Row(ID=76, name=\"b'Will'\")\n",
      "Row(ID=77, name=\"b'Weyoun'\")\n",
      "Row(ID=78, name=\"b'Data'\")\n",
      "Row(ID=79, name=\"b'Leeta'\")\n",
      "Row(ID=80, name=\"b'Dukat'\")\n",
      "Row(ID=81, name=\"b'Jadzia'\")\n",
      "Row(ID=82, name=\"b'Hugh'\")\n",
      "Row(ID=83, name=\"b'Geordi'\")\n",
      "Row(ID=84, name=\"b'Ben'\")\n",
      "Row(ID=85, name=\"b'Quark'\")\n",
      "Row(ID=86, name=\"b'Hugh'\")\n",
      "Row(ID=87, name=\"b'Ezri'\")\n",
      "Row(ID=88, name=\"b'Ben'\")\n",
      "Row(ID=89, name=\"b'Worf'\")\n",
      "Row(ID=90, name=\"b'Kasidy'\")\n",
      "Row(ID=91, name=\"b'Rom'\")\n",
      "Row(ID=92, name=\"b'Gowron'\")\n",
      "Row(ID=93, name=\"b'Elim'\")\n",
      "Row(ID=94, name=\"b'Morn'\")\n",
      "Row(ID=95, name=\"b'Odo'\")\n",
      "Row(ID=96, name=\"b'Ezri'\")\n",
      "Row(ID=97, name=\"b'Nerys'\")\n",
      "Row(ID=98, name=\"b'Will'\")\n",
      "Row(ID=99, name=\"b'Keiko'\")\n",
      "Row(ID=100, name=\"b'Jean-Luc'\")\n",
      "Row(ID=101, name=\"b'Morn'\")\n",
      "Row(ID=102, name=\"b'Dukat'\")\n",
      "Row(ID=103, name=\"b'Ezri'\")\n",
      "Row(ID=104, name=\"b'Dukat'\")\n",
      "Row(ID=105, name=\"b'Morn'\")\n",
      "Row(ID=106, name=\"b'Beverly'\")\n",
      "Row(ID=107, name=\"b'Will'\")\n",
      "Row(ID=108, name=\"b'Leeta'\")\n",
      "Row(ID=109, name=\"b'Quark'\")\n",
      "Row(ID=110, name=\"b'Nog'\")\n",
      "Row(ID=111, name=\"b'Nerys'\")\n",
      "Row(ID=112, name=\"b'Morn'\")\n",
      "Row(ID=113, name=\"b'Quark'\")\n",
      "Row(ID=114, name=\"b'Worf'\")\n",
      "Row(ID=115, name=\"b'Dukat'\")\n",
      "Row(ID=116, name=\"b'Ben'\")\n",
      "Row(ID=117, name=\"b'Rom'\")\n",
      "Row(ID=118, name=\"b'Ben'\")\n",
      "Row(ID=119, name=\"b'Worf'\")\n",
      "Row(ID=120, name=\"b'Jean-Luc'\")\n",
      "Row(ID=121, name=\"b'Deanna'\")\n",
      "Row(ID=122, name=\"b'Data'\")\n",
      "Row(ID=123, name=\"b'Jadzia'\")\n",
      "Row(ID=124, name=\"b'Data'\")\n",
      "Row(ID=125, name=\"b'Rom'\")\n",
      "Row(ID=126, name=\"b'Brunt'\")\n",
      "Row(ID=127, name=\"b'Miles'\")\n",
      "Row(ID=128, name=\"b'Julian'\")\n",
      "Row(ID=129, name=\"b'Kasidy'\")\n",
      "Row(ID=130, name=\"b'Gowron'\")\n",
      "Row(ID=131, name=\"b'Hugh'\")\n",
      "Row(ID=132, name=\"b'Odo'\")\n",
      "Row(ID=133, name=\"b'Quark'\")\n",
      "Row(ID=134, name=\"b'Ben'\")\n",
      "Row(ID=135, name=\"b'Rom'\")\n",
      "Row(ID=136, name=\"b'Will'\")\n",
      "Row(ID=137, name=\"b'Martok'\")\n",
      "Row(ID=138, name=\"b'Dukat'\")\n",
      "Row(ID=139, name=\"b'Nog'\")\n",
      "Row(ID=140, name=\"b'Elim'\")\n",
      "Row(ID=141, name=\"b'Miles'\")\n",
      "Row(ID=142, name=\"b'Guinan'\")\n",
      "Row(ID=143, name=\"b'Data'\")\n",
      "Row(ID=144, name=\"b'Miles'\")\n",
      "Row(ID=145, name=\"b'Leeta'\")\n",
      "Row(ID=146, name=\"b'Nerys'\")\n",
      "Row(ID=147, name=\"b'Quark'\")\n",
      "Row(ID=148, name=\"b'Nerys'\")\n",
      "Row(ID=149, name=\"b'Beverly'\")\n",
      "Row(ID=150, name=\"b'Nerys'\")\n",
      "Row(ID=151, name=\"b'Keiko'\")\n",
      "Row(ID=152, name=\"b'Kasidy'\")\n",
      "Row(ID=153, name=\"b'Morn'\")\n",
      "Row(ID=154, name=\"b'Geordi'\")\n",
      "Row(ID=155, name=\"b'Beverly'\")\n",
      "Row(ID=156, name=\"b'Deanna'\")\n",
      "Row(ID=157, name=\"b'Dukat'\")\n",
      "Row(ID=158, name=\"b'Odo'\")\n",
      "Row(ID=159, name=\"b'Kasidy'\")\n",
      "Row(ID=160, name=\"b'Beverly'\")\n",
      "Row(ID=161, name=\"b'Odo'\")\n",
      "Row(ID=162, name=\"b'Ezri'\")\n",
      "Row(ID=163, name=\"b'Nerys'\")\n",
      "Row(ID=164, name=\"b'Will'\")\n",
      "Row(ID=165, name=\"b'Leeta'\")\n",
      "Row(ID=166, name=\"b'Lwaxana'\")\n",
      "Row(ID=167, name=\"b'Quark'\")\n",
      "Row(ID=168, name=\"b'Martok'\")\n",
      "Row(ID=169, name=\"b'Beverly'\")\n",
      "Row(ID=170, name=\"b'Jean-Luc'\")\n",
      "Row(ID=171, name=\"b'Weyoun'\")\n",
      "Row(ID=172, name=\"b'Kasidy'\")\n",
      "Row(ID=173, name=\"b'Leeta'\")\n",
      "Row(ID=174, name=\"b'Deanna'\")\n",
      "Row(ID=175, name=\"b'Will'\")\n",
      "Row(ID=176, name=\"b'Worf'\")\n",
      "Row(ID=177, name=\"b'Brunt'\")\n",
      "Row(ID=178, name=\"b'Kasidy'\")\n",
      "Row(ID=179, name=\"b'Elim'\")\n",
      "Row(ID=180, name=\"b'Kasidy'\")\n",
      "Row(ID=181, name=\"b'Rom'\")\n",
      "Row(ID=182, name=\"b'Weyoun'\")\n",
      "Row(ID=183, name=\"b'Ben'\")\n",
      "Row(ID=184, name=\"b'Julian'\")\n",
      "Row(ID=185, name=\"b'Weyoun'\")\n",
      "Row(ID=186, name=\"b'Miles'\")\n",
      "Row(ID=187, name=\"b'Nerys'\")\n",
      "Row(ID=188, name=\"b'Keiko'\")\n",
      "Row(ID=189, name=\"b'Quark'\")\n",
      "Row(ID=190, name=\"b'Geordi'\")\n",
      "Row(ID=191, name=\"b'Nog'\")\n",
      "Row(ID=192, name=\"b'Jadzia'\")\n",
      "Row(ID=193, name=\"b'Nerys'\")\n",
      "Row(ID=194, name=\"b'Kasidy'\")\n",
      "Row(ID=195, name=\"b'Ben'\")\n",
      "Row(ID=196, name=\"b'Data'\")\n",
      "Row(ID=197, name=\"b'Leeta'\")\n",
      "Row(ID=198, name=\"b'Brunt'\")\n",
      "Row(ID=199, name=\"b'Hugh'\")\n",
      "Row(ID=200, name=\"b'Kasidy'\")\n",
      "Row(ID=201, name=\"b'Ezri'\")\n",
      "Row(ID=202, name=\"b'Lwaxana'\")\n",
      "Row(ID=203, name=\"b'Ezri'\")\n",
      "Row(ID=204, name=\"b'Deanna'\")\n",
      "Row(ID=205, name=\"b'Morn'\")\n",
      "Row(ID=206, name=\"b'Will'\")\n",
      "Row(ID=207, name=\"b'Lwaxana'\")\n",
      "Row(ID=208, name=\"b'Nog'\")\n",
      "Row(ID=209, name=\"b'Brunt'\")\n",
      "Row(ID=210, name=\"b'Data'\")\n",
      "Row(ID=211, name=\"b'Ben'\")\n",
      "Row(ID=212, name=\"b'Geordi'\")\n",
      "Row(ID=213, name=\"b'Worf'\")\n",
      "Row(ID=214, name=\"b'Miles'\")\n",
      "Row(ID=215, name=\"b'Will'\")\n",
      "Row(ID=216, name=\"b'Brunt'\")\n",
      "Row(ID=217, name=\"b'Keiko'\")\n",
      "Row(ID=218, name=\"b'Gowron'\")\n",
      "Row(ID=219, name=\"b'Lwaxana'\")\n",
      "Row(ID=220, name=\"b'Jean-Luc'\")\n",
      "Row(ID=221, name=\"b'Dukat'\")\n",
      "Row(ID=222, name=\"b'Rom'\")\n",
      "Row(ID=223, name=\"b'Odo'\")\n",
      "Row(ID=224, name=\"b'Keiko'\")\n",
      "Row(ID=225, name=\"b'Elim'\")\n",
      "Row(ID=226, name=\"b'Lwaxana'\")\n",
      "Row(ID=227, name=\"b'Ezri'\")\n",
      "Row(ID=228, name=\"b'Martok'\")\n",
      "Row(ID=229, name=\"b'Gowron'\")\n",
      "Row(ID=230, name=\"b'Beverly'\")\n",
      "Row(ID=231, name=\"b'Ezri'\")\n",
      "Row(ID=232, name=\"b'Worf'\")\n",
      "Row(ID=233, name=\"b'Gowron'\")\n",
      "Row(ID=234, name=\"b'Deanna'\")\n",
      "Row(ID=235, name=\"b'Elim'\")\n",
      "Row(ID=236, name=\"b'Brunt'\")\n",
      "Row(ID=237, name=\"b'Nerys'\")\n",
      "Row(ID=238, name=\"b'Deanna'\")\n",
      "Row(ID=239, name=\"b'Nog'\")\n",
      "Row(ID=240, name=\"b'Deanna'\")\n",
      "Row(ID=241, name=\"b'Quark'\")\n",
      "Row(ID=242, name=\"b'Data'\")\n",
      "Row(ID=243, name=\"b'Martok'\")\n",
      "Row(ID=244, name=\"b'Dukat'\")\n",
      "Row(ID=245, name=\"b'Jean-Luc'\")\n",
      "Row(ID=246, name=\"b'Leeta'\")\n",
      "Row(ID=247, name=\"b'Ezri'\")\n",
      "Row(ID=248, name=\"b'Dukat'\")\n",
      "Row(ID=249, name=\"b'Nerys'\")\n",
      "Row(ID=250, name=\"b'Hugh'\")\n",
      "Row(ID=251, name=\"b'Rom'\")\n",
      "Row(ID=252, name=\"b'Will'\")\n",
      "Row(ID=253, name=\"b'Leeta'\")\n",
      "Row(ID=254, name=\"b'Ezri'\")\n",
      "Row(ID=255, name=\"b'Deanna'\")\n",
      "Row(ID=256, name=\"b'Worf'\")\n",
      "Row(ID=257, name=\"b'Data'\")\n",
      "Row(ID=258, name=\"b'Worf'\")\n",
      "Row(ID=259, name=\"b'Kasidy'\")\n",
      "Row(ID=260, name=\"b'Brunt'\")\n",
      "Row(ID=261, name=\"b'Jadzia'\")\n",
      "Row(ID=262, name=\"b'Will'\")\n",
      "Row(ID=263, name=\"b'Martok'\")\n",
      "Row(ID=264, name=\"b'Julian'\")\n",
      "Row(ID=265, name=\"b'Gowron'\")\n",
      "Row(ID=266, name=\"b'Jean-Luc'\")\n",
      "Row(ID=267, name=\"b'Dukat'\")\n",
      "Row(ID=268, name=\"b'Ezri'\")\n",
      "Row(ID=269, name=\"b'Beverly'\")\n",
      "Row(ID=270, name=\"b'Data'\")\n",
      "Row(ID=271, name=\"b'Morn'\")\n",
      "Row(ID=272, name=\"b'Quark'\")\n",
      "Row(ID=273, name=\"b'Data'\")\n",
      "Row(ID=274, name=\"b'Julian'\")\n",
      "Row(ID=275, name=\"b'Will'\")\n",
      "Row(ID=276, name=\"b'Dukat'\")\n",
      "Row(ID=277, name=\"b'Hugh'\")\n",
      "Row(ID=278, name=\"b'Data'\")\n",
      "Row(ID=279, name=\"b'Beverly'\")\n",
      "Row(ID=280, name=\"b'Nerys'\")\n",
      "Row(ID=281, name=\"b'Worf'\")\n",
      "Row(ID=282, name=\"b'Geordi'\")\n",
      "Row(ID=283, name=\"b'Dukat'\")\n",
      "Row(ID=284, name=\"b'Nog'\")\n",
      "Row(ID=285, name=\"b'Data'\")\n",
      "Row(ID=286, name=\"b'Lwaxana'\")\n",
      "Row(ID=287, name=\"b'Beverly'\")\n",
      "Row(ID=288, name=\"b'Hugh'\")\n",
      "Row(ID=289, name=\"b'Ezri'\")\n",
      "Row(ID=290, name=\"b'Beverly'\")\n",
      "Row(ID=291, name=\"b'Dukat'\")\n",
      "Row(ID=292, name=\"b'Nog'\")\n",
      "Row(ID=293, name=\"b'Deanna'\")\n",
      "Row(ID=294, name=\"b'Leeta'\")\n",
      "Row(ID=295, name=\"b'Nerys'\")\n",
      "Row(ID=296, name=\"b'Data'\")\n",
      "Row(ID=297, name=\"b'Ben'\")\n",
      "Row(ID=298, name=\"b'Guinan'\")\n",
      "Row(ID=299, name=\"b'Rom'\")\n",
      "Row(ID=300, name=\"b'Beverly'\")\n",
      "Row(ID=301, name=\"b'Weyoun'\")\n",
      "Row(ID=302, name=\"b'Beverly'\")\n",
      "Row(ID=303, name=\"b'Deanna'\")\n",
      "Row(ID=304, name=\"b'Will'\")\n",
      "Row(ID=305, name=\"b'Quark'\")\n",
      "Row(ID=306, name=\"b'Beverly'\")\n",
      "Row(ID=307, name=\"b'Keiko'\")\n",
      "Row(ID=308, name=\"b'Morn'\")\n",
      "Row(ID=309, name=\"b'Geordi'\")\n",
      "Row(ID=310, name=\"b'Odo'\")\n",
      "Row(ID=311, name=\"b'Martok'\")\n",
      "Row(ID=312, name=\"b'Jadzia'\")\n",
      "Row(ID=313, name=\"b'Dukat'\")\n",
      "Row(ID=314, name=\"b'Geordi'\")\n",
      "Row(ID=315, name=\"b'Weyoun'\")\n",
      "Row(ID=316, name=\"b'Hugh'\")\n",
      "Row(ID=317, name=\"b'Guinan'\")\n",
      "Row(ID=318, name=\"b'Deanna'\")\n",
      "Row(ID=319, name=\"b'Leeta'\")\n",
      "Row(ID=320, name=\"b'Worf'\")\n",
      "Row(ID=321, name=\"b'Data'\")\n",
      "Row(ID=322, name=\"b'Rom'\")\n",
      "Row(ID=323, name=\"b'Nog'\")\n",
      "Row(ID=324, name=\"b'Miles'\")\n",
      "Row(ID=325, name=\"b'Keiko'\")\n",
      "Row(ID=326, name=\"b'Beverly'\")\n",
      "Row(ID=327, name=\"b'Julian'\")\n",
      "Row(ID=328, name=\"b'Deanna'\")\n",
      "Row(ID=329, name=\"b'Dukat'\")\n",
      "Row(ID=330, name=\"b'Ezri'\")\n",
      "Row(ID=331, name=\"b'Martok'\")\n",
      "Row(ID=332, name=\"b'Julian'\")\n",
      "Row(ID=333, name=\"b'Ben'\")\n",
      "Row(ID=334, name=\"b'Leeta'\")\n",
      "Row(ID=335, name=\"b'Odo'\")\n",
      "Row(ID=336, name=\"b'Gowron'\")\n",
      "Row(ID=337, name=\"b'Miles'\")\n",
      "Row(ID=338, name=\"b'Will'\")\n",
      "Row(ID=339, name=\"b'Morn'\")\n",
      "Row(ID=340, name=\"b'Nerys'\")\n",
      "Row(ID=341, name=\"b'Data'\")\n",
      "Row(ID=342, name=\"b'Guinan'\")\n",
      "Row(ID=343, name=\"b'Odo'\")\n",
      "Row(ID=344, name=\"b'Data'\")\n",
      "Row(ID=345, name=\"b'Ezri'\")\n",
      "Row(ID=346, name=\"b'Hugh'\")\n",
      "Row(ID=347, name=\"b'Geordi'\")\n",
      "Row(ID=348, name=\"b'Weyoun'\")\n",
      "Row(ID=349, name=\"b'Kasidy'\")\n",
      "Row(ID=350, name=\"b'Nog'\")\n",
      "Row(ID=351, name=\"b'Keiko'\")\n",
      "Row(ID=352, name=\"b'Deanna'\")\n",
      "Row(ID=353, name=\"b'Julian'\")\n",
      "Row(ID=354, name=\"b'Kasidy'\")\n",
      "Row(ID=355, name=\"b'Keiko'\")\n",
      "Row(ID=356, name=\"b'Weyoun'\")\n",
      "Row(ID=357, name=\"b'Brunt'\")\n",
      "Row(ID=358, name=\"b'Will'\")\n",
      "Row(ID=359, name=\"b'Nog'\")\n",
      "Row(ID=360, name=\"b'Nerys'\")\n",
      "Row(ID=361, name=\"b'Worf'\")\n",
      "Row(ID=362, name=\"b'Ezri'\")\n",
      "Row(ID=363, name=\"b'Dukat'\")\n",
      "Row(ID=364, name=\"b'Lwaxana'\")\n",
      "Row(ID=365, name=\"b'Brunt'\")\n",
      "Row(ID=366, name=\"b'Keiko'\")\n",
      "Row(ID=367, name=\"b'Data'\")\n",
      "Row(ID=368, name=\"b'Elim'\")\n",
      "Row(ID=369, name=\"b'Quark'\")\n",
      "Row(ID=370, name=\"b'Jadzia'\")\n",
      "Row(ID=371, name=\"b'Guinan'\")\n",
      "Row(ID=372, name=\"b'Brunt'\")\n",
      "Row(ID=373, name=\"b'Quark'\")\n",
      "Row(ID=374, name=\"b'Nog'\")\n",
      "Row(ID=375, name=\"b'Hugh'\")\n",
      "Row(ID=376, name=\"b'Gowron'\")\n",
      "Row(ID=377, name=\"b'Beverly'\")\n",
      "Row(ID=378, name=\"b'Guinan'\")\n",
      "Row(ID=379, name=\"b'Data'\")\n",
      "Row(ID=380, name=\"b'Brunt'\")\n",
      "Row(ID=381, name=\"b'Nog'\")\n",
      "Row(ID=382, name=\"b'Worf'\")\n",
      "Row(ID=383, name=\"b'Data'\")\n",
      "Row(ID=384, name=\"b'Martok'\")\n",
      "Row(ID=385, name=\"b'Beverly'\")\n",
      "Row(ID=386, name=\"b'Data'\")\n",
      "Row(ID=387, name=\"b'Will'\")\n",
      "Row(ID=388, name=\"b'Nog'\")\n",
      "Row(ID=389, name=\"b'Weyoun'\")\n",
      "Row(ID=390, name=\"b'Martok'\")\n",
      "Row(ID=391, name=\"b'Ben'\")\n",
      "Row(ID=392, name=\"b'Data'\")\n",
      "Row(ID=393, name=\"b'Quark'\")\n",
      "Row(ID=394, name=\"b'Keiko'\")\n",
      "Row(ID=395, name=\"b'Jean-Luc'\")\n",
      "Row(ID=396, name=\"b'Keiko'\")\n",
      "Row(ID=397, name=\"b'Quark'\")\n",
      "Row(ID=398, name=\"b'Lwaxana'\")\n",
      "Row(ID=399, name=\"b'Beverly'\")\n",
      "Row(ID=400, name=\"b'Data'\")\n",
      "Row(ID=401, name=\"b'Jean-Luc'\")\n",
      "Row(ID=402, name=\"b'Hugh'\")\n",
      "Row(ID=403, name=\"b'Weyoun'\")\n",
      "Row(ID=404, name=\"b'Kasidy'\")\n",
      "Row(ID=405, name=\"b'Nog'\")\n",
      "Row(ID=406, name=\"b'Ben'\")\n",
      "Row(ID=407, name=\"b'Miles'\")\n",
      "Row(ID=408, name=\"b'Dukat'\")\n",
      "Row(ID=409, name=\"b'Nog'\")\n",
      "Row(ID=410, name=\"b'Odo'\")\n",
      "Row(ID=411, name=\"b'Lwaxana'\")\n",
      "Row(ID=412, name=\"b'Jadzia'\")\n",
      "Row(ID=413, name=\"b'Hugh'\")\n",
      "Row(ID=414, name=\"b'Martok'\")\n",
      "Row(ID=415, name=\"b'Quark'\")\n",
      "Row(ID=416, name=\"b'Nerys'\")\n",
      "Row(ID=417, name=\"b'Julian'\")\n",
      "Row(ID=418, name=\"b'Guinan'\")\n",
      "Row(ID=419, name=\"b'Kasidy'\")\n",
      "Row(ID=420, name=\"b'Jadzia'\")\n",
      "Row(ID=421, name=\"b'Will'\")\n",
      "Row(ID=422, name=\"b'Ben'\")\n",
      "Row(ID=423, name=\"b'Will'\")\n",
      "Row(ID=424, name=\"b'Keiko'\")\n",
      "Row(ID=425, name=\"b'Quark'\")\n",
      "Row(ID=426, name=\"b'Worf'\")\n",
      "Row(ID=427, name=\"b'Brunt'\")\n",
      "Row(ID=428, name=\"b'Lwaxana'\")\n",
      "Row(ID=429, name=\"b'Jean-Luc'\")\n",
      "Row(ID=430, name=\"b'Geordi'\")\n",
      "Row(ID=431, name=\"b'Quark'\")\n",
      "Row(ID=432, name=\"b'Brunt'\")\n",
      "Row(ID=433, name=\"b'Ezri'\")\n",
      "Row(ID=434, name=\"b'Deanna'\")\n",
      "Row(ID=435, name=\"b'Guinan'\")\n",
      "Row(ID=436, name=\"b'Geordi'\")\n",
      "Row(ID=437, name=\"b'Morn'\")\n",
      "Row(ID=438, name=\"b'Nog'\")\n",
      "Row(ID=439, name=\"b'Data'\")\n",
      "Row(ID=440, name=\"b'Elim'\")\n",
      "Row(ID=441, name=\"b'Jadzia'\")\n",
      "Row(ID=442, name=\"b'Nog'\")\n",
      "Row(ID=443, name=\"b'Geordi'\")\n",
      "Row(ID=444, name=\"b'Keiko'\")\n",
      "Row(ID=445, name=\"b'Guinan'\")\n",
      "Row(ID=446, name=\"b'Quark'\")\n",
      "Row(ID=447, name=\"b'Julian'\")\n",
      "Row(ID=448, name=\"b'Nerys'\")\n",
      "Row(ID=449, name=\"b'Data'\")\n",
      "Row(ID=450, name=\"b'Weyoun'\")\n",
      "Row(ID=451, name=\"b'Martok'\")\n",
      "Row(ID=452, name=\"b'Ezri'\")\n",
      "Row(ID=453, name=\"b'Julian'\")\n",
      "Row(ID=454, name=\"b'Weyoun'\")\n",
      "Row(ID=455, name=\"b'Elim'\")\n",
      "Row(ID=456, name=\"b'Ezri'\")\n",
      "Row(ID=457, name=\"b'Worf'\")\n",
      "Row(ID=458, name=\"b'Elim'\")\n",
      "Row(ID=459, name=\"b'Geordi'\")\n",
      "Row(ID=460, name=\"b'Hugh'\")\n",
      "Row(ID=461, name=\"b'Geordi'\")\n",
      "Row(ID=462, name=\"b'Nerys'\")\n",
      "Row(ID=463, name=\"b'Keiko'\")\n",
      "Row(ID=464, name=\"b'Beverly'\")\n",
      "Row(ID=465, name=\"b'Morn'\")\n",
      "Row(ID=466, name=\"b'Brunt'\")\n",
      "Row(ID=467, name=\"b'Weyoun'\")\n",
      "Row(ID=468, name=\"b'Ezri'\")\n",
      "Row(ID=469, name=\"b'Brunt'\")\n",
      "Row(ID=470, name=\"b'Deanna'\")\n",
      "Row(ID=471, name=\"b'Brunt'\")\n",
      "Row(ID=472, name=\"b'Nog'\")\n",
      "Row(ID=473, name=\"b'Morn'\")\n",
      "Row(ID=474, name=\"b'Keiko'\")\n",
      "Row(ID=475, name=\"b'Ben'\")\n",
      "Row(ID=476, name=\"b'Jean-Luc'\")\n",
      "Row(ID=477, name=\"b'Gowron'\")\n",
      "Row(ID=478, name=\"b'Dukat'\")\n",
      "Row(ID=479, name=\"b'Kasidy'\")\n",
      "Row(ID=480, name=\"b'Geordi'\")\n",
      "Row(ID=481, name=\"b'Elim'\")\n",
      "Row(ID=482, name=\"b'Ben'\")\n",
      "Row(ID=483, name=\"b'Hugh'\")\n",
      "Row(ID=484, name=\"b'Leeta'\")\n",
      "Row(ID=485, name=\"b'Rom'\")\n",
      "Row(ID=486, name=\"b'Elim'\")\n",
      "Row(ID=487, name=\"b'Brunt'\")\n",
      "Row(ID=488, name=\"b'Nog'\")\n",
      "Row(ID=489, name=\"b'Odo'\")\n",
      "Row(ID=490, name=\"b'Nerys'\")\n",
      "Row(ID=491, name=\"b'Rom'\")\n",
      "Row(ID=492, name=\"b'Dukat'\")\n",
      "Row(ID=493, name=\"b'Hugh'\")\n",
      "Row(ID=494, name=\"b'Kasidy'\")\n",
      "Row(ID=495, name=\"b'Data'\")\n",
      "Row(ID=496, name=\"b'Gowron'\")\n",
      "Row(ID=497, name=\"b'Lwaxana'\")\n",
      "Row(ID=498, name=\"b'Jadzia'\")\n",
      "Row(ID=499, name=\"b'Leeta'\")\n"
     ]
    }
   ],
   "source": [
    "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
    "for teen in teenagers.collect():\n",
    "    print(teen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77dcf541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 18|    8|\n",
      "| 19|   11|\n",
      "| 20|    5|\n",
      "| 21|    8|\n",
      "| 22|    7|\n",
      "| 23|   10|\n",
      "| 24|    5|\n",
      "| 25|   11|\n",
      "| 26|   17|\n",
      "| 27|    8|\n",
      "| 28|   10|\n",
      "| 29|   12|\n",
      "| 30|   11|\n",
      "| 31|    8|\n",
      "| 32|   11|\n",
      "| 33|   12|\n",
      "| 34|    6|\n",
      "| 35|    8|\n",
      "| 36|   10|\n",
      "| 37|    9|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also use functions instead of SQL queries:\n",
    "schemaPeople.groupBy(\"age\").count().orderBy(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25ef81c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|       name|age|\n",
      "+-----------+---+\n",
      "|    b'Hugh'| 55|\n",
      "|  b'Deanna'| 40|\n",
      "|   b'Quark'| 68|\n",
      "|  b'Weyoun'| 59|\n",
      "|  b'Gowron'| 37|\n",
      "|    b'Will'| 54|\n",
      "|  b'Jadzia'| 38|\n",
      "|     b'Odo'| 53|\n",
      "|     b'Ben'| 57|\n",
      "|   b'Keiko'| 54|\n",
      "|b'Jean-Luc'| 56|\n",
      "|    b'Hugh'| 43|\n",
      "|     b'Rom'| 36|\n",
      "|     b'Odo'| 35|\n",
      "|b'Jean-Luc'| 45|\n",
      "|  b'Geordi'| 60|\n",
      "|     b'Odo'| 67|\n",
      "|   b'Keiko'| 51|\n",
      "|   b'Leeta'| 42|\n",
      "|  b'Martok'| 49|\n",
      "+-----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemaPeople.select(\"name\",\"age\").filter(\"age > 33\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee39efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
